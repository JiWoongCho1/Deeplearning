{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5849f72b",
   "metadata": {},
   "source": [
    "# Seq2Seq 내용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400a0424",
   "metadata": {},
   "source": [
    "**데이터전처리**\n",
    "\n",
    "**spacy 라이브러리 이용**\n",
    "\n",
    "**토큰화, 태깅화 유용**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "7e7b9809",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "08ab4bc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in c:\\users\\choij\\anaconda3\\lib\\site-packages (3.2.2)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in c:\\users\\choij\\anaconda3\\lib\\site-packages (from spacy) (8.0.13)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in c:\\users\\choij\\anaconda3\\lib\\site-packages (from spacy) (0.4.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\choij\\anaconda3\\lib\\site-packages (from spacy) (2.11.3)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\choij\\anaconda3\\lib\\site-packages (from spacy) (2.0.6)\n",
      "Requirement already satisfied: pathy>=0.3.5 in c:\\users\\choij\\anaconda3\\lib\\site-packages (from spacy) (0.6.1)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in c:\\users\\choij\\anaconda3\\lib\\site-packages (from spacy) (0.7.6)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\choij\\anaconda3\\lib\\site-packages (from spacy) (1.19.5)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in c:\\users\\choij\\anaconda3\\lib\\site-packages (from spacy) (3.0.8)\n",
      "Requirement already satisfied: setuptools in c:\\users\\choij\\anaconda3\\lib\\site-packages (from spacy) (52.0.0.post20210125)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\choij\\anaconda3\\lib\\site-packages (from spacy) (3.0.6)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\choij\\anaconda3\\lib\\site-packages (from spacy) (2.0.6)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\choij\\anaconda3\\lib\\site-packages (from spacy) (1.0.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\choij\\anaconda3\\lib\\site-packages (from spacy) (4.59.0)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in c:\\users\\choij\\anaconda3\\lib\\site-packages (from spacy) (2.4.2)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\choij\\anaconda3\\lib\\site-packages (from spacy) (1.0.6)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in c:\\users\\choij\\anaconda3\\lib\\site-packages (from spacy) (0.9.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\choij\\anaconda3\\lib\\site-packages (from spacy) (20.9)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\choij\\anaconda3\\lib\\site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in c:\\users\\choij\\anaconda3\\lib\\site-packages (from spacy) (1.8.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\choij\\anaconda3\\lib\\site-packages (from spacy) (2.25.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\choij\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy) (2.4.7)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in c:\\users\\choij\\anaconda3\\lib\\site-packages (from pathy>=0.3.5->spacy) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\choij\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy) (3.7.4.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\choij\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\choij\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\choij\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\choij\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.12.5)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\choij\\anaconda3\\lib\\site-packages (from typer<0.5.0,>=0.3.0->spacy) (7.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\choij\\anaconda3\\lib\\site-packages (from jinja2->spacy) (1.1.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.4; however, version 22.0.3 is available.\n",
      "You should consider upgrading via the 'c:\\users\\choij\\anaconda3\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "d08e4fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en import English\n",
    "from spacy.lang.de import German"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "13ac247f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_en = English()#영어 토큰화\n",
    "nlp_de = German() # 독일어 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "797f9f22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "인덱스 : 0, 토큰 : i\n",
      "인덱스 : 1, 토큰 : am\n",
      "인덱스 : 2, 토큰 : a\n",
      "인덱스 : 3, 토큰 : graduate\n",
      "인덱스 : 4, 토큰 : student\n",
      "인덱스 : 5, 토큰 : .\n"
     ]
    }
   ],
   "source": [
    "tokenized = nlp_en.tokenizer(\"i am a graduate student.\")\n",
    "\n",
    "for i, token in enumerate(tokenized):\n",
    "    print(\"인덱스 : {}, 토큰 : {}\".format(i, token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "63d75076",
   "metadata": {},
   "outputs": [],
   "source": [
    "#독일어를 토큰화 한 후에 뒤집어주는 함수\n",
    "def tokenize_de(text):\n",
    "    return [token.text for token in nlp_de.tokenizer(text)][::-1]\n",
    "\n",
    "def tokenize_en(text): # 영어를 토큰화해주는 함수\n",
    "    return [token.text for token in nlp_en.tokenizer(text)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ba8086",
   "metadata": {},
   "source": [
    "**Field 라이브러리 이용해서 데이터셋에 대한 구체적인 전처리 내용 명시**\n",
    "\n",
    "**번역 소스 : 독일어**\n",
    "    \n",
    "**번역 목표 : 영어**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "838bf43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.legacy.data import Field, BucketIterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "be307bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC = Field(tokenize = tokenize_de, init_token = \"<sos>\", eos_token = \"<eos>\", lower = True)\n",
    "TRG = Field(tokenize = tokenize_en, init_token = \"<sos>\", eos_token = \"<eos>\", lower = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e92ccc34",
   "metadata": {},
   "source": [
    "**대표적인 영어-독어 데이터셋인 Multi30K 불러온다**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "de972679",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.legacy.datasets import Multi30k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "3dfb2cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, valid_dataset, test_dataset = Multi30k.splits(exts = (\".de\", \".en\"), fields = (SRC, TRG))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "df67f6f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습 데이터 셋 크기: 29000\n",
      "평가 데이터 셋 크기: 1014\n",
      "테스트 데이터 셋 크기 : 1000\n"
     ]
    }
   ],
   "source": [
    "print(\"학습 데이터 셋 크기: {}\".format(len(train_dataset.examples)))\n",
    "print(\"평가 데이터 셋 크기: {}\".format(len(valid_dataset.examples)))\n",
    "print(\"테스트 데이터 셋 크기 : {}\".format(len(test_dataset.examples)))            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "7ac71354",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.', 'steht', 'urinal', 'einem', 'an', 'kaffee', 'tasse', 'einer', 'mit', 'der', ',', 'mann', 'ein']\n",
      "['a', 'man', 'standing', 'at', 'a', 'urinal', 'with', 'a', 'coffee', 'cup', '.']\n"
     ]
    }
   ],
   "source": [
    "print(vars(train_dataset.examples[30])['src'])\n",
    "print(vars(train_dataset.examples[30])['trg'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8f8785",
   "metadata": {},
   "source": [
    "**필드 객체의 build_vocab 이용하여 단어 사전 생성**\n",
    "\n",
    "**최소 2번 이상의 단어만 선택**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "1328a85f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(SRC) : 7853\n",
      "len(TRG) : 5893\n"
     ]
    }
   ],
   "source": [
    "SRC.build_vocab(train_dataset, min_freq = 2)\n",
    "TRG.build_vocab(train_dataset, min_freq = 2)\n",
    "\n",
    "print(\"len(SRC) : {}\".format(len(SRC.vocab)))\n",
    "print(\"len(TRG) : {}\".format(len(TRG.vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "c8d2d494",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4112\n",
      "1752\n"
     ]
    }
   ],
   "source": [
    "print(TRG.vocab.stoi['abcde']) #없는 단어\n",
    "print(TRG.vocab.stoi[TRG.pad_token]) #padding : 인덱스 1 차지\n",
    "print(TRG.vocab.stoi['<sos>']) # sos, eos 는 인덱스 각각 2,3 차지\n",
    "print(TRG.vocab.stoi['<eos>'])\n",
    "print(TRG.vocab.stoi['hello'])\n",
    "print(TRG.vocab.stoi['world'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "aa2e0b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73175a85",
   "metadata": {},
   "source": [
    "**한 문장에 포함된 단어가 연속적으로 LSTM에 입력되어야함**\n",
    "\n",
    "**따라서 하나의 배치에 포함된 문장들이 가지는 단어의 개수가 유사하도록 만들면 좋음**\n",
    "\n",
    "**이를위해 BucketIterator 를 사용**\n",
    "\n",
    "**배치크기 : 128**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "9ded212c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "batch_size = 128\n",
    "#일반적인 데이터로더의 iterator 와 유사하게 사용 가능\n",
    "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
    "                            (train_dataset, valid_dataset, test_dataset),\n",
    "                            batch_size = batch_size,\n",
    "                            device = device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "ad039b68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "첫 번째 배치 크기 : torch.Size([33, 128])\n",
      "인덱스 : 0 : 2\n",
      "인덱스 : 1 : 4\n",
      "인덱스 : 2 : 162\n",
      "인덱스 : 3 : 46\n",
      "인덱스 : 4 : 14\n",
      "인덱스 : 5 : 12\n",
      "인덱스 : 6 : 31\n",
      "인덱스 : 7 : 128\n",
      "인덱스 : 8 : 257\n",
      "인덱스 : 9 : 6\n",
      "인덱스 : 10 : 7\n",
      "인덱스 : 11 : 16\n",
      "인덱스 : 12 : 890\n",
      "인덱스 : 13 : 8\n",
      "인덱스 : 14 : 3\n",
      "인덱스 : 15 : 1\n",
      "인덱스 : 16 : 1\n",
      "인덱스 : 17 : 1\n",
      "인덱스 : 18 : 1\n",
      "인덱스 : 19 : 1\n",
      "인덱스 : 20 : 1\n",
      "인덱스 : 21 : 1\n",
      "인덱스 : 22 : 1\n",
      "인덱스 : 23 : 1\n",
      "인덱스 : 24 : 1\n",
      "인덱스 : 25 : 1\n",
      "인덱스 : 26 : 1\n",
      "인덱스 : 27 : 1\n",
      "인덱스 : 28 : 1\n",
      "인덱스 : 29 : 1\n",
      "인덱스 : 30 : 1\n",
      "인덱스 : 31 : 1\n",
      "인덱스 : 32 : 1\n"
     ]
    }
   ],
   "source": [
    "for i, batch in enumerate(train_iterator):\n",
    "    src = batch.src\n",
    "    trg = batch.trg\n",
    "    \n",
    "    print(\"첫 번째 배치 크기 : {}\".format(src.shape))\n",
    "    \n",
    "    #현재 배치에 있는 하나의 문장에 포함된 정보 출력\n",
    "    for i in range(src.shape[0]):\n",
    "        print(\"인덱스 : {} : {}\".format(i, src[i][0].item()))\n",
    "        \n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95ba6f8",
   "metadata": {},
   "source": [
    "**인코더(Encoder) 아키텍처**\n",
    "\n",
    "**LSTM은 hidden state 과 cell state 반환**\n",
    "\n",
    "**주어진 소스 문장을 문맥 벡터(context vector)로 인코딩**\n",
    "\n",
    "**하이퍼 파라미터**\n",
    "\n",
    "**input_dim : 하나의 단어에 대한 원핫 인코딩 차원**\n",
    "\n",
    "**embed_dim : 임베딩 차원**\n",
    "    \n",
    "**hidden_dim : 히든상태 차원**\n",
    "    \n",
    "**n_layers : RNN레이어의 개수**\n",
    "    \n",
    "**dropout_ratio : 드롭아웃 비율**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "5ba3afa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "a665b257",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, embed_dim, hidden_dim, n_layers, dropout_ratio):\n",
    "        super().__init__()\n",
    "        #인베딩은 원-핫 인코딩을 특정 차원의 임베딩으로 매핑하는 레이어\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_dim, embed_dim)\n",
    "        \n",
    "        #LSTM레이어\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.rnn = nn.LSTM(embed_dim, hidden_dim, n_layers, dropout = dropout_ratio)\n",
    "        \n",
    "        #드롭아웃\n",
    "        self.dropout = nn.Dropout(dropout_ratio)\n",
    "        \n",
    "        \n",
    "        #인코더는 소스 문장을 입력으로 받아 문맥 벡터를 반환\n",
    "        def forward(self, src):\n",
    "            #src : [단어개수, 배치크기] : 각 단어으 ㅣ인덱스 정보\n",
    "            embedded = self.dropout(self.embedding(src))\n",
    "            #embedded : [단어의 개수, 배치크기, 임베딩 차원]\n",
    "            \n",
    "            outputs, (hidden, cell) = self.rnn(embedded)\n",
    "            #outputs : [단어개수, 배치크기, 히든차원] : 현재 단어의 출력 정보\n",
    "            #hidden : [레이어 개수, 배치크기, 히든 차원] : 현재까지의 모든 단어의 정보\n",
    "            #cell : [레이어 개수, 배치 크기, 히든 차원] : 현재까지의 모든 단어의 정보\n",
    "            \n",
    "            #문맥 벡터 반환\n",
    "            return hidden, cell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22213648",
   "metadata": {},
   "source": [
    "**디코더(Decoder) 아키텍처**\n",
    "\n",
    "**LSTM은 hidden state 과 cell state 반환**\n",
    "\n",
    "**주어진 문맥 벡터를 타겟 문장으로 (target vector)로 디코딩**\n",
    "\n",
    "**하이퍼 파라미터**\n",
    "\n",
    "**input_dim : 하나의 단어에 대한 원핫 인코딩 차원**\n",
    "\n",
    "**embed_dim : 임베딩 차원**\n",
    "    \n",
    "**hidden_dim : 히든상태 차원**\n",
    "    \n",
    "**n_layers : RNN레이어의 개수**\n",
    "    \n",
    "**dropout_ratio : 드롭아웃 비율**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "7685014d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, embed_dim, hidden_dim, n_layers, dropout_ratio):\n",
    "        super().__init__()\n",
    "        #인베딩은 원-핫 인코딩을 특정 차원의 임베딩으로 매핑하는 레이어\n",
    "        \n",
    "        self.embedding = nn.Embedding(output_dim, embed_dim)\n",
    "        \n",
    "        #LSTM레이어\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.rnn = nn.LSTM(embed_dim, hidden_dim, n_layers, dropout = dropout_ratio)\n",
    "        \n",
    "        #FC 레이어 (인코더와 구조적으로 다른 부분)\n",
    "        self.output_dim = output_dim\n",
    "        self.fc_out = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "        #드롭아웃\n",
    "        self.dropout = nn.Dropout(dropout_ratio)\n",
    "        \n",
    "        \n",
    "        #인코더는 소스 문장을 입력으로 받아 문맥 벡터를 반환\n",
    "        def forward(self, input, hidden, cell):\n",
    "            #input : [배치크기] : 단어의 개수는 항상 1개이도록 구현\n",
    "            #hidden : [레이어 개수, 배치크기, 히든 차원]\n",
    "            #cell : context : [레이어개수, 배치크기, 히든차원]\n",
    "            \n",
    "            input = input.unsqueeze(0)\n",
    "            #input : [단어개수 = 1, 배치크기]\n",
    "            \n",
    "            embedded = self.dropout(self.embedding(input))\n",
    "            #embedded : [단어개수, 배치크기, 임베딩차원]\n",
    "            \n",
    "            output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n",
    "            #outputs : [단어개수, 배치크기, 히든차원] : 현재 단어의 출력 정보\n",
    "            #hidden : [레이어 개수, 배치크기, 히든 차원] : 현재까지의 모든 단어의 정보\n",
    "            #cell : [레이어 개수, 배치 크기, 히든 차원] : 현재까지의 모든 단어의 정보\n",
    "            \n",
    "            prediction = self.fc_out(output.squeeze(0))\n",
    "            #prediction = [배치크기, 출력차원]\n",
    "            \n",
    "            #(현재 출력 단어, 현재까지의 모든 단어의 정보, 현재까지의 모든 단어의 정보)\n",
    "            return prediction, hidden, cell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef1de79",
   "metadata": {},
   "source": [
    "**Seq2Seq 아키텍처**\n",
    "\n",
    "**앞서 정의한 인코더와 디코더를 가지고 있는 하나의 아키텍처**\n",
    "\n",
    "**인코더 : 주어진 소스 문장을 문맥 벡터로 인코딩**\n",
    "\n",
    "**디코더 : 주어진 문맥 벡터를 타겟 문장으로 디코딩**\n",
    "\n",
    "**단 디코더는 한 단어씩 넣어서 한 번씩 결과를 구한다**\n",
    "\n",
    "**Teacher forcing : 디코더의 예측(prediction)을 다음 입력으로 사용하지 않고 실제 목표 출력(ground-truth)를 다음 입력으로 사용하는 기법**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "c73e7d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        \n",
    "    #학습할 때는 완전한 형태의 소스 문장, 타겟 문장, teacher_forcing_ratio 를 넣기\n",
    "    def forward(self, src, trg, teacher_forcing_ratio = 0.5):\n",
    "        #src : [단어개수, 배치크기]\n",
    "        #trg : [단어 개수, 배치크기]\n",
    "        #먼저 인코더를 거쳐 문맥벡터를 추출\n",
    "        hidden, cell = self.encoder(src)\n",
    "        \n",
    "        #디코더의 최종 결과를 담을 텐서 객체 만들기\n",
    "        trg_len = trg.shape[0] #단어 개수\n",
    "        batch_size = trg.shape[1] #배치 크기\n",
    "        trg_vocab_size = self.decoder.output_dim #출력 차원\n",
    "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
    "        \n",
    "        #첫번째 입력은 항상 <sos>토큰\n",
    "        input = trg[0, :]\n",
    "        \n",
    "        #타겟 단어의 개수만큼 반복하여 디코더에 포워딩(forwarding)\n",
    "        for t in range(1, trg_len):\n",
    "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
    "            \n",
    "            outputs[t] = output # FC 를 거쳐서 나온 현재의 출력 단어 정보\n",
    "            top1 = output.argmax(1) # 가장 확률이 높은 단어의 인덱스 추출\n",
    "            \n",
    "            #teacher_forcing_ratio : 학습할 때 실제 목표 출력(ground-truth)을 사용하는 비율\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            input = trg[t] if teacher_force else top1 #현재의 출력 결과를 다음 입력에서 넣기\n",
    "            \n",
    "        return outputs\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03a8be5",
   "metadata": {},
   "source": [
    "**학습 (Training)**\n",
    "\n",
    "**하이퍼파라미터 설정 및 모델 초기화**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "691c5fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(SRC.vocab)\n",
    "OUTPUT_DIM = len(TRG.vocab)\n",
    "ENCODER_EMBED_DIM = 256\n",
    "DECODER_EMBED_DIM = 256\n",
    "HIDDEN_DIM = 512\n",
    "N_LAYERS = 2\n",
    "ENC_DROPOUT_RATIO = 0.5\n",
    "DEC_DROPOUT_RATIO = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "64aeae38",
   "metadata": {},
   "outputs": [],
   "source": [
    "#인코더와 디코더 객체 선언\n",
    "enc = Encoder(INPUT_DIM, ENCODER_EMBED_DIM, HIDDEN_DIM, N_LAYERS, ENC_DROPOUT_RATIO)\n",
    "dec = Decoder(OUTPUT_DIM, DECODER_EMBED_DIM, HIDDEN_DIM, N_LAYERS, DEC_DROPOUT_RATIO)\n",
    "\n",
    "#Seq2Seq 객체 선언\n",
    "model = Seq2Seq(enc, dec, device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "00063b47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(7853, 256)\n",
       "    (rnn): LSTM(256, 512, num_layers=2, dropout=0.5)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (embedding): Embedding(5893, 256)\n",
       "    (rnn): LSTM(256, 512, num_layers=2, dropout=0.5)\n",
       "    (fc_out): Linear(in_features=512, out_features=5893, bias=True)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.uniform_(param.data, -0.08, 0.08)\n",
    "    \n",
    "model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "76100d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "d40fc4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adam optimizer 로 학습\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "#뒷부분의 패딩에 대해서는 값 무시\n",
    "TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "2d7bb2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#모델 학습 함수\n",
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    #전체 학습 데이터 확인\n",
    "    for i, batch in enumerate(iterator):\n",
    "        src = batch.src\n",
    "        trg = batch.trg\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(src, trg)\n",
    "        #output : [출력단어개수, 배치크기, 출력차원]\n",
    "        output_dim = output.shape[-1]\n",
    "        \n",
    "        #출력 단어의 인덱스 0은 사용하지 않음\n",
    "        output = output[1:].view(-1, output_dim)\n",
    "        #output = [(출력단어의 개수-1) * batch size, output dim]\n",
    "        trg = trg[1:].view(-1)\n",
    "        #모델의 출력 결과와 타겟 문장을 비교하여 손실 계산\n",
    "        loss = criterion(output, trg)\n",
    "        loss.backward() #기울기 계산\n",
    "        \n",
    "        #기울기 clipping 진행\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        #파라미터 업데이트\n",
    "        optimizer.step()\n",
    "        \n",
    "        #전체 손실 값 계산\n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "faf0020c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#모델 평가 함수\n",
    "def evaluate(model, iterator, criterion):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        #전체 평가 데이터를 확인하며\n",
    "        for i, batch in enumerate(iterator):\n",
    "            src = batch.src\n",
    "            trg = batch.trg\n",
    "            \n",
    "            #평가할 때 teacher forcing 사용  x\n",
    "            output = model(src, trg, 0)\n",
    "            output_dim = output.shape[-1]\n",
    "            \n",
    "            output = output[1:].view(-1, output_dim)\n",
    "            trg = trg[1:].view(-1)\n",
    "            \n",
    "            loss = criterion(output, trg)\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e546b8c",
   "metadata": {},
   "source": [
    "**학습 및 검증 진행**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "ff7f14c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "003bdbb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "6e4bacc6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-152-568640317247>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#시작시간 기록\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mtrain_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_iterator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCLIP\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[0mvalid_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_iterator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-148-1f342b5d80be>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, iterator, optimizer, criterion, clip)\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m         \u001b[1;31m#output : [출력단어개수, 배치크기, 출력차원]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0moutput_dim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-142-ba3dfd33f37a>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, src, trg, teacher_forcing_ratio)\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[1;31m#trg : [단어 개수, 배치크기]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[1;31m#먼저 인코더를 거쳐 문맥벡터를 추출\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m         \u001b[0mhidden\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcell\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[1;31m#디코더의 최종 결과를 담을 텐서 객체 만들기\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_forward_unimplemented\u001b[1;34m(self, *input)\u001b[0m\n\u001b[0;32m    199\u001b[0m         \u001b[0mregistered\u001b[0m \u001b[0mhooks\u001b[0m \u001b[1;32mwhile\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mlatter\u001b[0m \u001b[0msilently\u001b[0m \u001b[0mignores\u001b[0m \u001b[0mthem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m     \"\"\"\n\u001b[1;32m--> 201\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    202\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 20\n",
    "CLIP = 1\n",
    "bast_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    start_time = time.time() #시작시간 기록\n",
    "    \n",
    "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
    "    valid_loss = evaluate(model, valid_iterator, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'seq2seq.pt')\n",
    "    \n",
    "    print(f'Epoch : {epoch + 1:02} | time : {epoch_mins}m {epochs_secs}s')\n",
    "    print(f'\\tTrain Loss : {train_loss : .3f} | Train PPL : {math.exp(train_loss):.3f}')\n",
    "    print(f'\\tValidation Loss : {valid_loss :.3f} | Validation PPL : {math.exp(valid_loss):.3f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
